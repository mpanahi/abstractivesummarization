{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersianAbstractiveSummarization_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUBiQQlmTzxvmVWDHY3Ylq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpanahi/abstractivesummarization/blob/main/PersianAbstractiveSummarization_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIcrn6npKWSH"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "7QFrxgEwKmDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install folium==0.2.1"
      ],
      "metadata": {
        "id": "vyqWAJOOSiu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "onMfx9SuSlbL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "data = load_dataset(\"pn_summary\")"
      ],
      "metadata": {
        "id": "8GqG53XzSvK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = data[\"train\"]\n",
        "print(len(train))\n",
        "indices = np.random.randint(0, len(train), 10).tolist()\n",
        "df = pd.DataFrame(train)\n",
        "df = df[[\"id\", \"title\", \"article\", \"summary\", \"category\", \"categories\", \"network\", \"link\"]]\n",
        "df[\"article\"] = df[\"article\"].apply(lambda t: t.replace('[n]', ' ')[:512] + ' [...]')\n",
        "df[\"category\"] = df[\"category\"].apply(lambda t: train.features[\"category\"].int2str(t))\n",
        "df[\"network\"] = df[\"network\"].apply(lambda t: train.features[\"network\"].int2str(t))\n",
        "train_data=[]\n",
        "for i in range(len(df)):\n",
        "  dc={}\n",
        "  dc[\"article_original\"]=df[\"article\"][i]\n",
        "  dc[\"abstractive\"]=df[\"summary\"][i]\n",
        "  train_data.append(dc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otMOp9WsS6VD",
        "outputId": "b633a12f-8a59-401e-bf6f-51a5d388a91f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_dataset(batch_size):\n",
        "    \n",
        "\n",
        "    train_set = []\n",
        "    for data in train_data:\n",
        "        \n",
        "        article_original = data[\"article_original\"].replace(\"\\n\", \" \")\n",
        "        abstractive = data[\"abstractive\"].replace(\"\\n\", \" \")\n",
        "        train_set.append((article_original, abstractive))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=32,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader"
      ],
      "metadata": {
        "id": "SUrkXKveTC0U"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.10\n",
        "!pip install hazm\n",
        "!pip install kobert_transformers\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "Chb_b-rjTOva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "\n",
        "from pytorch_lightning import Trainer\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class LightningBase(pl.LightningModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_save_path: str,\n",
        "            max_len: int,\n",
        "            batch_size: int,\n",
        "            num_gpus: int,\n",
        "            max_epochs:int=4,\n",
        "            min_epochs:int=1,\n",
        "            lr: float = 3e-5,\n",
        "            weight_decay: float = 1e-4,\n",
        "            save_step_interval: int = 1000,\n",
        "            accelerator: str = \"dp\",\n",
        "            precision: int = 16,\n",
        "            use_amp: bool = True,\n",
        "    ) -> None:\n",
        "        \"\"\"constructor of LightningBase\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.model_save_path = model_save_path\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.weight_decay = weight_decay\n",
        "        self.max_len = max_len\n",
        "        self.num_gpus = num_gpus\n",
        "        self.max_epochs=4,\n",
        "        self.min_epochs=1,\n",
        "        self.save_step_interval = save_step_interval\n",
        "        self.accelerator = accelerator\n",
        "        self.precision = precision\n",
        "        self.use_amp = use_amp\n",
        "        self.model = None\n",
        "        \n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"configure optimizers and lr schedulers\"\"\"\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        model = self.model\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": self.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        self.optimizer = AdamW(\n",
        "            params=optimizer_grouped_parameters,\n",
        "            lr=self.lr,\n",
        "            weight_decay=self.weight_decay,\n",
        "        )\n",
        "\n",
        "        return [self.optimizer]\n",
        "\n",
        "    def fit(self, train_dataloader: DataLoader):\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            gpus=self.num_gpus,\n",
        "            \n",
        "            precision=self.precision,\n",
        "            max_epochs=self.max_epochs[0],\n",
        "            min_epochs=self.min_epochs[0],\n",
        "            amp_backend=\"native\" if self.use_amp else None,\n",
        "        )\n",
        "\n",
        "        trainer.fit(\n",
        "            model=self, train_dataloaders=train_dataloader\n",
        "        )\n",
        "        \n",
        "\n",
        "    def save_model(self) -> None:\n",
        "        if (\n",
        "                self.trainer.global_rank == 0\n",
        "                and self.global_step % self.save_step_interval == 0\n",
        "        ):\n",
        "            torch.save(\n",
        "                self.model.state_dict(),\n",
        "                self.model_save_path + \".\" + str(self.global_step),\n",
        "            )"
      ],
      "metadata": {
        "id": "3TJuK2nETfBi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from transformers import (\n",
        "    EncoderDecoderModel,\n",
        "    BertConfig,\n",
        "    EncoderDecoderConfig,\n",
        "    BertModel, BertTokenizer,\n",
        ")\n",
        "\n",
        "#from transformers.modeling_bart import shift_tokens_right\n",
        "from kobert_transformers import get_tokenizer\n",
        "import pytorch_lightning\n",
        "import torch\n",
        "from  transformers import AutoTokenizer\n",
        "#from lightning_base import LightningBase\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
        "    prev_output_tokens = input_ids.clone()\n",
        "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "    return prev_output_tokens\n",
        "class Bert2Bert(LightningBase):\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_save_path: str,\n",
        "            batch_size: int,\n",
        "            num_gpus: int,\n",
        "            max_epochs:int=4,\n",
        "            min_epochs=1,\n",
        "            max_len: int = 512,\n",
        "            lr: float = 3e-5,\n",
        "            weight_decay: float = 1e-4,\n",
        "            save_step_interval: int = 1000,\n",
        "            accelerator: str = \"dp\",\n",
        "            precision: int = 16,\n",
        "            use_amp: bool = True,\n",
        "    ) -> None:\n",
        "        super(Bert2Bert, self).__init__(\n",
        "            model_save_path=model_save_path,\n",
        "            max_len=max_len,\n",
        "            max_epochs=4,\n",
        "            min_epochs=1,\n",
        "            batch_size=batch_size,\n",
        "            num_gpus=num_gpus,\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            save_step_interval=save_step_interval,\n",
        "            use_amp=use_amp,\n",
        "            precision=precision\n",
        "            \n",
        "        )\n",
        "        encoder_config = BertConfig.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\")\n",
        "        decoder_config = BertConfig.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\")\n",
        "        config = EncoderDecoderConfig.from_encoder_decoder_configs(\n",
        "            encoder_config, decoder_config\n",
        "        )\n",
        "\n",
        "        self.model = EncoderDecoderModel(config)\n",
        "        self.tokenizer = KoBertTokenizer()\n",
        "        #self.tokenizer =AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\")\n",
        "        state_dict = BertModel.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\").state_dict()\n",
        "        self.model.encoder.load_state_dict(state_dict)\n",
        "        self.model.decoder.bert.load_state_dict(state_dict, strict=False)\n",
        "        \n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch[0], batch[1]\n",
        "        \n",
        "        src_input = self.tokenizer.encode_batch(src, max_length=self.max_len)\n",
        "        \n",
        "        tgt_input = self.tokenizer.encode_batch(tgt, max_length=self.max_len)\n",
        "        \n",
        "        input_ids = src_input[\"input_ids\"].to(self.device)\n",
        "        attention_mask = src_input[\"attention_mask\"].to(self.device)\n",
        "        labels = tgt_input[\"input_ids\"].to(self.device)\n",
        "        decoder_input_ids = shift_tokens_right(\n",
        "            labels, self.tokenizer.token2idx(\"[PAD]\")\n",
        "        )\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "        )\n",
        "\n",
        "        lm_logits = outputs[0]\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(\n",
        "            ignore_index=self.tokenizer.token2idx(\"[PAD]\")\n",
        "        )\n",
        "\n",
        "        lm_loss = loss_fn(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n",
        "        self.save_model()\n",
        "        return {\"loss\": lm_loss}\n",
        "\n",
        "\n",
        "class KoBertTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\")\n",
        "        self.token2idx = self.tokenizer.convert_tokens_to_ids\n",
        "        self.idx2token = self.tokenizer.convert_ids_to_tokens\n",
        "\n",
        "    def encode_batch(self, x: List[str], max_length):\n",
        "        max_len = 0\n",
        "        result_tokenization = []\n",
        "\n",
        "        for i in x:\n",
        "            tokens = self.tokenizer.encode(i, max_length=max_length, truncation=True)\n",
        "            result_tokenization.append(tokens)\n",
        "\n",
        "            if len(tokens) > max_len:\n",
        "                max_len = len(tokens)\n",
        "\n",
        "        padded_tokens = []\n",
        "        for tokens in result_tokenization:\n",
        "            padding = (torch.ones(max_len) * self.token2idx(\"[PAD]\")).long()\n",
        "            padding[: len(tokens)] = torch.tensor(tokens).long()\n",
        "            padded_tokens.append(padding.unsqueeze(0))\n",
        "\n",
        "        padded_tokens = torch.cat(padded_tokens, dim=0).long()\n",
        "        mask_tensor = torch.ones(padded_tokens.size()).long()\n",
        "\n",
        "        attention_mask = torch.where(\n",
        "            padded_tokens == self.token2idx(\"[PAD]\"), padded_tokens, mask_tensor * -1\n",
        "        ).long()\n",
        "        attention_mask = torch.where(\n",
        "            attention_mask == -1, attention_mask, mask_tensor * 0\n",
        "        ).long()\n",
        "        attention_mask = torch.where(\n",
        "            attention_mask != -1, attention_mask, mask_tensor\n",
        "        ).long()\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": padded_tokens.long(),\n",
        "            \"attention_mask\": attention_mask.long(),\n",
        "        }\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \n",
        "        tokens = [token for token in tokens\n",
        "                  if token not in [0, 1, 2, 3, 4]]\n",
        "\n",
        "        decoded = [self.idx2token(token) for token in tokens]\n",
        "        if \"▁\" in decoded[0] and \"▁\" in decoded[1]:\n",
        "            # fix decoding bugs\n",
        "            tokens = tokens[1:]\n",
        "\n",
        "        return self.tokenizer.decode(tokens)\n",
        "\n",
        "    def decode_batch(self, list_of_tokens):\n",
        "        return [self.decode(tokens) for tokens in list_of_tokens]"
      ],
      "metadata": {
        "id": "U15rnqbcTyRR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Bert2Bert(\n",
        "        model_save_path=\"model_summary_persian_new.pt\",\n",
        "        batch_size=16,\n",
        "        num_gpus=1,\n",
        "        max_epochs=4,\n",
        "        min_epochs=1\n",
        "    )\n",
        "\n",
        "train = load_dataset(batch_size=trainer.batch_size)\n",
        "trainer.fit(train)"
      ],
      "metadata": {
        "id": "mnd5hrq3cLRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}